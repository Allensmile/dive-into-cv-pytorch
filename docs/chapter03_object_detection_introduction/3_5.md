# 3.5 损失函数

## 3.5.1 Matching strategy (匹配策略)：

在训练时，ground truth boxes 与 default boxe（就是prior boxes） 按照如下方式进行配对：

**第一个原则：**从ground truth box出发，寻找与每一个ground truth box有最大的jaccard overlap的default box，这样就能保证每一个groundtruth box一定与一个default box对应起来（所谓的jaccard overlap就是IOU，如图2.5.1-1）。 反之，若一个prior box没有与任何ground truth进行匹配，那么该prior box只能与背景匹配，就是负样本。一个图片中ground truth是非常少的， 而default box却很多，如果仅按第一个原则匹配，很多default box会是负样本，正负样本极其不平衡，所以需要第二个原则。

**第二个原则：**从default box出发，对剩余的还没有配对的default box与任意一个ground truth box尝试配对，只要两者之间的jaccard overlap大于阈值（一般是0.5），那么该default box也与这个ground truth进行匹配。这意味着某个ground truth可能与多个Prior box匹配，这是可以的。但是反过来却不可以，因为一个prior box只能匹配一个ground truth，如果多个ground truth与某个prior box的 IOU 大于阈值，那么default box只与IOU最大的那个Prior box进行匹配。注意：第二个原则一定在第一个原则之后进行，仔细考虑一下这种情况，如果某个ground truth所对应最大 IOU小于阈值，并且所匹配的Prior box却与另外一个ground truth的 IOU大于阈值，那么该Prior box应该匹配谁，答案应该是前者，首先要确保某个ground truth一定有一个Prior box与之匹配。但是，这种情况我觉得基本上是不存在的。由于Prior box很多，某个ground truth的最大 IOU 肯定大于阈值，所以可能只实施第二个原则既可以了。显然配对到GT的default box就是positive，没有配对到GT的default box就是negative。只有正样本才会参与loss的计算。

<div align=center>
<img src="https://raw.githubusercontent.com/datawhalechina/dive-into-cv-pytorch/master/markdown_imgs/chapter03/2-26.png">
</div>
<center>图2-26 IOU图示</center>
<center>$IOU=\frac{A∩B}{A∪B}$<center>


用一个示例来说明上述的匹配原则：

<div align=center>
<img src="https://raw.githubusercontent.com/datawhalechina/dive-into-cv-pytorch/master/markdown_imgs/chapter03/2-27.png">
</div>
<center>图2-27</center>

为了方便采用7个先验框在图像中是红色的框，黄色的是ground truths，在这幅图像中有三个真实的目标。按照前面列出的步骤将生成以下匹配项：

<div align=center>
<img src="https://raw.githubusercontent.com/datawhalechina/dive-into-cv-pytorch/master/markdown_imgs/chapter03/2-28.png">
</div>
<center>图2-28</center>

每个先验都有一个匹配，正的或负的。相对应的每个预测都有一个匹配，正的或负的。

### 3.5.2 损失函数

 损失函数 Loss计算：总体目标损失函数定位损失（loc）和置信度损失（conf）的加权和：

$L(x,c,l,g) = \frac{1}{N}(L_{conf}(x,c)+\alpha L_{loc} (x,l,g)) (1)$

SSD算法的目标函数分为两部分：计算相应的default box与目标类别的**confidence loss**以及相应的**location loss**(位置回归)。其中N是匹配到GT（Ground Truth）的prior box数量，如果N=0，则将损失设为0；而 **α** 参数用于调整confidence loss和location loss之间的比例，默认 **α=1**。SSD中的confidence loss是在多类别置信度(c)上的softmax loss，公式如下：

$L_{conf}(x,c) = -\sum_{i \in Pos}^N x^{p}_{ij} log(\hat{c}^{i}_{p}) - \sum_{i \in Neg} log(\hat{c}^{0}_{i})  Where \hat{c}^{p}_{i} = \frac{exp(c^{p}_{i})}{\sum_p exp(c^{p}_{i})} (2)$

其中i指代搜索框序号，j指代真实框序号，p指代类别序号，p=0表示背景。其中$x^{p}_{ij}=\left\{1,0\right\}$ 中取1表示第i个prior box匹配到第 j 个GT box，而这个GT box的类别为 p 。$x^{p}_{i}$ 表示第i个搜索框对应类别p的预测概率。此处有一点需要关注，公式前半部分是正样本（Pos）的损失，即分类为某个类别的损失（不包括背景），后半部分是负样本（Neg）的损失，也就是类别为背景的损失。

而location loss（位置回归）是典型的smooth L1 loss

$L_{loc}(x,l,g) = \sum_{i \in Pos  m \in \left\{c_x,c_y,w,h\right\}}^N \sum x^{k}_{ij} smooth_{L1}(l^{m}_{i}-\hat{g}^{m}_{j}) (3)$

$\hat{g}^{c_x}_{j}=(g^{c_x}_{j}-d^{c_x}_{i})/d^{w}_{i}$

$\hat{g}^{c_y}_{j}=(g^{c_y}_{j}-d^{c_y}_{i})/d^{h}_{i}$

$\hat{g}^{w}_{j}=log(\frac{g^{w}_{j}}{d^{w}_{i}})$

$\hat{g}^{h}_{j}=log(\frac{g^{h}_{j}}{d^{h}_{i}})$

其中，l为预测框，g为ground truth。(cx,xy)为补偿(regress to offsets)后的默认框d的中心,(w,h)为默认框的宽和高。更详细的解释看-看下图：

此处还缺一张图，记得上传

此部分的代码见model.py中的class MultiBoxLoss(nn.module)这个类。self.smooth_l1 = nn.L1Loss()   #smooth_l1损失定位损失
self.cross_entropy = nn.CrossEntropyLoss(reduce=False)   #交叉熵损失分类损失。

### 3.5.3 Hard negative mining:

这里我们的 Tiny-Detector 与 SSD 一样的损失函数与难例挖掘策略。

值得注意的是，一般情况下negative default boxes数量>>positive default boxes数量，直接训练会导致网络过于重视负样本，从而loss不稳定。为了保证正负样本尽量平衡，SSD在训练时采用了hard negative mining，即依据confidience loss对default box进行排序，挑选其中confidience loss高的box进行训练，将正负样本的比例控制在positive：negative=1:3。显而易见，用来训练网络的负样本为提取的负样本的子集，那么，我们当然选择负样本中容易被分错类的困难负样本来进行网络训练这样会取得更好的效果。有下面一种说法：SSD采用了hard negative mining，就是对负样本进行抽样，抽样时按照置信度误差(预测背景的置信度越小，误差越大)进行降序排列，选取误差的较大的top-k作为训练的负样本，以保证正负样本比例接近1:3。下面提供的是正负样本都做难例子挖掘。

具体过程如下：正负样本区分通过定位信息(iou)初步划分，而后通过loss把hard example留下，loss越大越hard。

1)正样本获得

已经确定 default box，结合 ground truth，下一步要将 default box 匹配到 ground truth 上，根据上面两个原则，从 groudtruth box 出发给每个 groudtruth box 找到了最近(IOU最大)的 default box 放入候选正样本集。然后再从 default box 出发为 default box 集中寻找与 groundtruth box 满足 IOU>0.5 的的default box放入候选正样本集。

2)负样本获得(这是一个难例挖掘的过程)

在生成 prior boxes 之后，会产生很多个符合 ground truth box 的 positive boxes(候选正样本集)，但同时，不符合 ground truth boxes 也很多，而且这个 negative boxes(候选负样本集)，远多于 positive boxes。这会造成 negative boxes、positive boxes 之间的不均衡。训练时难以收敛。

3)难例挖掘

将每一个GT上对应prior boxes的分类loss 进行排序。对于候选正样本集：选择loss最高的m个 prior box 与候选正样本集匹配 (box 索引同时存在于这两个集合里则匹配成功)，匹配不成功则从候选正样本集中删除这个正样本(因为这个正样本loss太低，易被识别，所以不在难例里，已经很接近 ground truth box 了，不需要再训练)；

对于候选负样本集：选择loss最高的m个prior box 与候选负样本集匹配，匹配成功的则留下来作为最后的负样本，不成功剔除出候选负样本集，因为他们loss不够大，易被识别为背景，训练起来没难度没提升空间。

4)举例：假设在这 441 个 default box 里，经过 FindMatches 后得到候选正样本 P 个，候选负样本那就有 441−P个。将 prior box 的 prediction loss 按照从大到小顺序排列后选择最高的 M个 prior box。如果这 P 个候选正样本里有 a 个 box 不在这 M 个 prior box 里，则将这 a个 box 从候选正样本集中踢出去。如果这 441−P个候选负样本集中有 b个在这 M 个 prior box，则将这b个候选负样本作为正式负样本。即删除易识别的正样本，同时留下典型的负样本，组成1:3的prior boxes样本集合。SSD 算法中通过这种方式来保证 positives、negatives 的比例。

5)Hard example mining

example mining 是选择出特定样本来计算损失函数；从实际问题出发 hard example 应该就是指定位较困难或分类较困难或者两者都困难的候选框。

Hard negative example 或 Hard positive example 的定义需要首先确定某个候选框是 negative example 还是 positive example。比如 SSD 中将与任意 gt_bbox 的 IOU 超过给定阈值 overlap_threshold（SSD 默认为 0.5）的当作正样本，即前景类别为正样本，背景类别为负样本。比如，极端的例子，当图像中没有 gt_bbox 时，那么所有的 default bboxes 都是 negative example。

SSD 中 negative mining 只计算分类损失而不计算定位损失（Tiny-Detector和SSD一样），而 hard example mining 对分类损失和定位损失都进行计算。

Tiny-Detector 的 negative mining 的过程为：

1)生成default bboxes，每个检测器对应特征图生成的default boxes数目为n*n*9；

2)匹配default boxes，将每个 default box 与 ground truth 匹配，保证每个ground truth 都能对应多个default boxes，避免漏检；

3)衡量default boxes，当第 i 个 default box 被匹配到第 j 个 gt_bbox，那么计算其属于背景类别的 softmax loss 或 cross entropy loss 值;

代码参考model.py中的class MultiBoxLoss(nn.module)：

```                                                                                                                                                python
 #Number of positive and hard-negative priors per image
 n_positives = positive_priors.sum(dim=1) # (N)
 n_hard_negatives = self.neg_pos_ratio * n_positives # (N)
 # First, find the loss for all priors
 conf_loss_all=self.cross_entropy(predicted_scores.view(-1,n_classes), true_classes.view(-1)) # (N * 441)
 conf_loss_all = conf_loss_all.view(batch_size, n_priors) # (N, 441)
```



