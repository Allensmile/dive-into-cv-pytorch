# 3.1 目标检测基本概念

### 3.1.1 什么是目标检测

​       目标检测是计算机视觉中的一个重要任务，近年来传统目标检测方法已经难以满足人们对目标检测效果的要求，随着深度学习在计算机视觉任务上取得的巨大进展，目前基于深度学习的目标检测算法已经成为主流。

​       相比较于基于深度学习的图像分类任务，目标检测任务更具难度。

​       具体区别如图3-1所示。

​       图像分类：只需要判断输入的图像中是否包含感兴趣物体。

​       目标检测：需要在识别出图片中目标类别的基础上，还要精确定位到目标的具体位置，并用外接矩形框标出。

<img src="https://raw.githubusercontent.com/datawhalechina/dive-into-cv-pytorch/master/markdown_imgs/chapter03/3-1.png">

​                                                                       图3-1   分类和目标检测任务示意图

### 3.1.2 目标检测的思路

​		自2012年Alex Krizhevsky凭借Alex在ImageNet图像分类挑战赛中拿下冠军之后，深度学习在图像识别尤其是图像分类领域开始大放异彩，大众的视野也重新回到深度神经网络中。紧接着，不断有更深更复杂的网络出现，一再刷新ImageNet图像分类比赛的记录。

​       但是人们发现，一张图像中很有可能包含不止一个物体或者不止一个类别的物体，单纯的图像分类网络在识别这些图像时显然是无法达到很好的效果。因此，人们就想，那如果知道了每个物体的位置，再把这个物体从原图中抠出来，再放入到分类网络中去进行分类，那我不就可以知道图像中每个物体的位置和类别了吗？

​        但是，怎么样才能知道每个物体的位置呢？显然我们是没办法知道的，但是我们可以去猜啊！所谓猜，其实就是一个个去试，把图像中的每个区域一个个抠出来，记下被抠出的区域的位置，再送入到分类网络进行分类得到其类别，这样对于图像中每个区域都能得到（class,x1,y1,x2,y2）五个属性，也就得到了被抠出区域的类别和外接矩形框坐标。再看一下图2-1，一不小心，我们就完成了图像的目标检测任务！

<img src="https://raw.githubusercontent.com/datawhalechina/dive-into-cv-pytorch/master/markdown_imgs/chapter03/3-2.png">

​                                                               图3-2  从分类角度去看目标检测

​        图3-1展示了一个通过遍历各个区域，然后逐个分类去完成目标检测任务的过程示例。在待识别图上预设一个框，然后逐像素遍历，就能得到一堆框（为了便于理解，图上只展示了3个框用于说明问题，具体数量由图像大小和预设框大小决定，理解意思就行），每个框送入到分类网络分类都有一个boat(图片中有一个boat目标)的得分，那么得分最高的就代表识别的最准确的框，其位置就是最终要检测的目标的位置。

​        以上就是最初的基于深度学习的目标检测问题解决思路。但是很显然，这种方法能做，但是效率会是一个非常大的问题。比如我们有一个100x100尺寸的图像，图片中目标长宽为20x20，因此预设一个20x20大小的预选区域，那么我们需要抠81x81次，执行81x81次分类才能得到一张图像上的检测结果，实际情况中并不知道目标长宽，因此问题会更加繁琐，显然实际应用时难以去运用这种方法。于是人们就想，是否能够通过传统图像处理的方法，根据像素之间的关系预定义一些可能含有目标的区域（Selective Search），只将这些区域送入分类网络得到区域的类别，然后通过回归的方法对这些区域外接矩形的坐标进行回归，从而大幅度缩短检测时间。这些预定义的区域就是我们目标检测领域常说的候选框，rcnn家族、yolo家族等众多经典目标检测模型都是在这个基础上发展起来的。

​        本文会基于以上思路，带领大家从0开始一步步搭建一个目标检测模型，并完成模型的训练测试及评价！



### 3.1.3 目标框定义方式

​        我们知道，图像分类任务中每张用于训练是数据主要包括两项，图片和类别（img，label）。目标检测也是一样，但也稍有不同，目标检测的label需要同时包含目标的类别和位置信息，如图3-3所示：

<img src="https://raw.githubusercontent.com/datawhalechina/dive-into-cv-pytorch/master/markdown_imgs/chapter03/3-3.png" alt="1604134486(1)" style="zoom:75%;" />

​                                                                           图3-3  目标框定义方式



### 3.1.4 交并比（IOU）

​        IOU的全称是交并比（Intersection over Union），表示两个目标框的交集占其并集的比例。在目标检测任务中，关于IOU的计算贯穿整个模型的训练测试和评价过程，是非常非常重要的一个概念，图3-4为IOU计算示意图。

<img src="https://raw.githubusercontent.com/datawhalechina/dive-into-cv-pytorch/master/markdown_imgs/chapter03/3-4.png" alt="1604134486(1)" style="zoom:50%;" />

​                                                                                                           图 3-4 IOU计算示意图

​       在了解IOU之前，大家需要知道什么是bounding_box，它指的就是目标的外接矩形框，一般简写为bbox，后面对目标外接矩形框的描述均以bbox表示。

图中可以看到，分子中黄色区域为红bbox和绿bbox的交集，分母中黄+红+绿区域为红bbox和绿bbox的并集，两者之比即为iou。

那么具体怎么去计算呢？这里给出计算流程，具体代码见项目代码***code/chapter03_object_detection_introduction/utils/utils.py***

```
1.首先定义红框坐标，左上：(red_x1, red_y1), 右下：(red_x2, red_y2).绿框坐标：左上(green_x1, green_y1)，右下:(green_x2, green_y2)
2.计算红绿框的面积：red_area和green_area
3.计算黄框（交集）坐标：左上:(max(red_x1, green_x1), max(red_y1, green_y1)), 右下:(min(red_x2, green_x2), min(red_y2, green_y2))
4.计算黄框面积：yellow_area
5.iou = yellow_area / (red_area+green_area-yellow_area)

*注意：1. iou计算的分母不要忘记减去两个框的并集
      2. 考虑两框不相交的情况，iou=0
```

