# 2.1 目标检测基本概念

### 2.1.1 什么是目标检测

​       目标检测是计算机视觉中的一个重要问题，近年来传统检测方法已经难以满足人们对目标检测效果的要求，随着深度学习在计算机视觉任务上取得的巨大进展，目前基于深度学习的目标检测算法已经成为主流。

​         相比较于基于深度学习的图像分类任务，目标检测任务更具难度。图像分类只需要判断输入的图像中是否包含感兴趣物体，而目标检测需要在识别出感兴趣物体的基础上，定位到物体的具体位置，这需要模型在识别图像中物体的同时，还要精确计算出每个物体的位置，如图2-1所示。

<img src="https://raw.githubusercontent.com/datawhalechina/dive-into-cv-pytorch/master/markdown_imgs/chapter03/2-1.png">

​                                                                       图2-1   分类和目标检测任务示意图

### 2.1.2 目标检测的思路

​		自2012年Alex Krizhevsky凭借Alex在ImageNet图像分类挑战赛中拿下冠军之后，深度学习在图像识别尤其是图像分类领域开始大放异彩，大众的视野也重新回到深度神经网络中。紧接着，不断有更深更复杂的网络出现，一再刷新ImageNet图像分类比赛的记录。

​       但是人们发现，一张图像中很有可能包含不止一个物体或者不止一个种类的物体，这样对于单纯的图像分类网络就变得比较棘手。因此，人们就想，那我如果知道了每个物体的位置，再把这个物体从原图中抠出来，再放入到分类网络中去进行分类，那我不就可以知道图像中每个物体的位置和类别了吗？

​        但是，怎么样才能知道每个物体的位置呢？显然我们是没办法知道的，但是我们可以去猜啊！所谓猜其实就是一个个去试，如图2-2所示，把图像中的每个区域一个个抠出来，记下被抠出的区域的位置，再送入到分类网络进行分类，即对于图像中每个区域都能得到（class,x1,y1,x2,y2）五个属性，就是被抠出区域的类别和外接矩形框坐标，一不小心，我们就完成了图像的目标检测任务！

<img src="https://raw.githubusercontent.com/datawhalechina/dive-into-cv-pytorch/master/markdown_imgs/chapter03/2-2.png">

​                                                               图2-2  从分类角度去看目标检测

   如图所示，在图上预设一个框，然后挨个像素遍历，就能得到一堆框（为了便于理解，图上只展示了3个框用于说明问题，具体数量由图像大小和预设框大小决定，理解意思就行），每个框送入到分类网络分类都有一个boat的得分，那么得分最高的就代表识别的最准确，其位置就是最终要检测的目标的位置。

​        以上就是目标检测问题最初的基于深度学习的解决思路。但是很显然，这种方法能做，但是效率会是一个非常大的问题。比如我们有一个100x100尺寸的图像，目标大小大约为20x20，那么我们需要抠81x81次预选区域，执行81x81次分类才能得到一张图像上的检测结果，显然难以去运用这种方法。于是人们就想，是否能够通过传统图像处理的方法，预定义一些可能含有目标的区域，只将这些区域送入分类网络得到区域的类别，然后通过回归的方法对这些区域外接矩形的坐标进行回归，从而大幅度缩短检测时间。这些预定义的区域就是我们目标检测领域常说的候选框，rcnn家族、yolo家族等众多经典目标检测模型都是在这个基础上发展起来的。

​        本文会基于以上思路，带领大家从0开始一步步搭建一个目标检测模型，并完成模型的训练测试及评价！



### 2.1.3 目标框定义方式

​        我们知道，图像分类任务中数据主要包括两项，图片和类别（img，label）。目标检测也是一样，但也稍有不同，目标检测的label需要同时包含目标的类别和位置信息，如下图所示：

<img src="https://raw.githubusercontent.com/datawhalechina/dive-into-cv-pytorch/master/markdown_imgs/chapter03/2-3.png" alt="1604134486(1)" style="zoom:75%;" />

​                                                                           图2-3  目标框定义方式



### 2.1.4 IOU

​        IOU的全称是交并比（Intersection over Union），表示两个目标框的交集占其并集的比例。在目标检测任务中，关于IOU的计算贯穿整个模型的训练测试和评价过程，是非常非常重要的一个概念，图2-4为IOU示意图。

<img src="C:\Users\Dell\AppData\Roaming\Typora\typora-user-images\image-20201104175330812.png" alt="image-20201104175330812" style="zoom:50%;" />

​                                                                          图 2-4 IOU计算示意图

​       在介绍这个图之前向大家介绍bounding_box的概念，指的就是目标的外接矩形框，一般简写为bbox，后面对目标外接矩形框的描述均以bbox表示。

图中可以看到，分子中黄色区域为红bbox和绿bbox的交集，分母中黄+红+绿区域为红bbox和绿bbox的并集，两者之比即为iou。

那么具体怎么去计算呢？这里给出计算流程，具体代码见项目代码***code/chapter03_object_detection_introduction/utils/utils.py***

```
1.首先定义红框坐标，左上：(red_x1, red_y1), 右下：(red_x2, red_y2).绿框坐标：左上(green_x1, green_y1)，右下:(green_x2, green_y2)
2.计算红绿框的面积：red_area和green_area
3.计算黄框（交集）坐标：左上:(max(red_x1, green_x1), max(red_y1, green_y1)), 右下:(min(red_x2, green_x2), min(red_y2, green_y2))
4.计算黄框面积：yellow_area
5.iou = yellow_area / (red_area+green_area-yellow_area)

*注意：1. iou计算的分母不要忘记减去两个框的并集
      2. 考虑两框不相交的情况，iou=0
```

